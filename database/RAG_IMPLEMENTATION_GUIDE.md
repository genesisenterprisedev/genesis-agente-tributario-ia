# ü§ñ Sistema RAG com Embeddings - Guia de Implementa√ß√£o

## üìã √çndice
1. [Vis√£o Geral](#vis√£o-geral)
2. [Configura√ß√£o do Banco de Dados](#configura√ß√£o-do-banco-de-dados)
3. [Arquitetura do Sistema](#arquitetura-do-sistema)
4. [Implementa√ß√£o Frontend](#implementa√ß√£o-frontend)
5. [Implementa√ß√£o Backend](#implementa√ß√£o-backend)
6. [Fluxo Completo](#fluxo-completo)
7. [Testes](#testes)

---

## üéØ Vis√£o Geral

Este sistema implementa **RAG (Retrieval-Augmented Generation)** usando:
- **Supabase** como banco vetorial
- **pgvector** para busca sem√¢ntica
- **OpenAI Embeddings** (ada-002, 1536 dimens√µes)
- **HNSW** para busca vetorial r√°pida

### Benef√≠cios:
‚úÖ Respostas contextualizadas baseadas em documentos
‚úÖ Busca sem√¢ntica (n√£o apenas keywords)
‚úÖ Escal√°vel (HNSW index)
‚úÖ Seguro (RLS policies)

---

## üóÑÔ∏è Configura√ß√£o do Banco de Dados

### 1. Execute o Script SQL

No **Supabase SQL Editor**, execute:

```bash
database/rag_embeddings_schema.sql
```

Este script cria:
- ‚úÖ Tabelas: `pdf_documents`, `pdf_chunks`, `document_embeddings`
- ‚úÖ √çndices HNSW para busca vetorial
- ‚úÖ Fun√ß√µes: `match_document_chunks()`, `match_pdf_chunks()`
- ‚úÖ Pol√≠ticas RLS para seguran√ßa
- ‚úÖ Views √∫teis para estat√≠sticas

### 2. Verifique a Instala√ß√£o

```sql
-- Verificar se pgvector est√° habilitado
SELECT * FROM pg_extension WHERE extname = 'vector';

-- Verificar tabelas criadas
SELECT tablename FROM pg_tables 
WHERE schemaname = 'public' 
AND tablename LIKE '%chunk%' OR tablename LIKE '%pdf%';

-- Verificar fun√ß√µes
SELECT proname FROM pg_proc 
WHERE proname LIKE 'match_%';
```

---

## üèóÔ∏è Arquitetura do Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    USU√ÅRIO                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              FRONTEND (Next.js + React)                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Upload PDF   ‚îÇ  ‚îÇ Chat Input   ‚îÇ  ‚îÇ Document     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ Sidebar      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              API ROUTES (Next.js)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ /api/embed   ‚îÇ  ‚îÇ /api/search  ‚îÇ  ‚îÇ /api/chat    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚ñº                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  OpenAI API      ‚îÇ          ‚îÇ  Supabase        ‚îÇ
‚îÇ  (Embeddings)    ‚îÇ          ‚îÇ  (pgvector)      ‚îÇ
‚îÇ                  ‚îÇ          ‚îÇ                  ‚îÇ
‚îÇ  - ada-002       ‚îÇ          ‚îÇ  - pdf_chunks    ‚îÇ
‚îÇ  - 1536 dims     ‚îÇ          ‚îÇ  - HNSW index    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üíª Implementa√ß√£o Frontend

### 1. Criar API Route para Embeddings

```typescript
// app/api/embed/route.ts
import { NextResponse } from 'next/server';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export async function POST(req: Request) {
  try {
    const { text } = await req.json();

    const response = await openai.embeddings.create({
      model: 'text-embedding-ada-002',
      input: text,
    });

    const embedding = response.data[0].embedding;

    return NextResponse.json({ embedding });
  } catch (error: any) {
    console.error('Embedding error:', error);
    return NextResponse.json(
      { error: error.message },
      { status: 500 }
    );
  }
}
```

### 2. Criar API Route para Busca Sem√¢ntica

```typescript
// app/api/search/route.ts
import { NextResponse } from 'next/server';
import { supabase } from '@/services/supabaseClient';

export async function POST(req: Request) {
  try {
    const { query, embedding, limit = 5, threshold = 0.7 } = await req.json();

    // Buscar chunks similares
    const { data, error } = await supabase.rpc('match_document_chunks', {
      query_embedding: embedding,
      match_threshold: threshold,
      match_count: limit,
    });

    if (error) throw error;

    return NextResponse.json({ results: data });
  } catch (error: any) {
    console.error('Search error:', error);
    return NextResponse.json(
      { error: error.message },
      { status: 500 }
    );
  }
}
```

### 3. Atualizar Chat para Usar RAG

```typescript
// services/geminiService.ts
export async function generateResponseWithRAG(
  userMessage: string,
  conversationHistory: Message[]
) {
  // 1. Gerar embedding da pergunta
  const embeddingResponse = await fetch('/api/embed', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text: userMessage }),
  });
  const { embedding } = await embeddingResponse.json();

  // 2. Buscar chunks relevantes
  const searchResponse = await fetch('/api/search', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ 
      query: userMessage, 
      embedding,
      limit: 5,
      threshold: 0.7
    }),
  });
  const { results } = await searchResponse.json();

  // 3. Construir contexto
  const context = results
    .map((r: any) => `[${r.document_name}]\n${r.chunk_text}`)
    .join('\n\n---\n\n');

  // 4. Gerar resposta com contexto
  const systemPrompt = `Voc√™ √© um assistente tribut√°rio especializado.
  
Use o seguinte contexto para responder a pergunta do usu√°rio:

${context}

Se a informa√ß√£o n√£o estiver no contexto, diga que n√£o tem certeza.`;

  // ... resto da l√≥gica de chat
}
```

---

## üîß Implementa√ß√£o Backend

### 1. Processar Documento e Gerar Embeddings

```typescript
// services/documentProcessor.ts
import { supabase } from './supabaseClient';

export async function processDocumentWithEmbeddings(
  documentId: string,
  chunks: { text: string; index: number }[]
) {
  for (const chunk of chunks) {
    // Gerar embedding
    const embeddingResponse = await fetch('/api/embed', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text: chunk.text }),
    });
    const { embedding } = await embeddingResponse.json();

    // Salvar no banco
    const { error } = await supabase
      .from('document_embeddings')
      .insert({
        document_id: documentId,
        chunk_index: chunk.index,
        chunk_text: chunk.text,
        embedding,
        metadata: {
          length: chunk.text.length,
          processed_at: new Date().toISOString(),
        },
      });

    if (error) {
      console.error('Error saving embedding:', error);
    }
  }
}
```

### 2. Atualizar `moveToKnowledgeBase`

```typescript
// store.ts
moveToKnowledgeBase: async (id) => {
  try {
    // 1. Atualizar documento
    const { error } = await supabase
      .from('documents')
      .update({ is_deletable: false })
      .match({ id });

    if (error) throw error;

    // 2. Obter documento
    const doc = get().documents.find(d => d.id === id);
    if (!doc) throw new Error('Documento n√£o encontrado');

    // 3. Processar chunks e gerar embeddings
    const chunks = chunkText(doc.content, 500); // 500 chars por chunk
    await processDocumentWithEmbeddings(id, chunks);

    // 4. Atualizar estado local
    set((state) => ({
      documents: state.documents.map((doc) =>
        doc.id === id ? { ...doc, isDeletable: false } : doc
      ),
    }));

    console.log('Documento adicionado √† base com embeddings!');
  } catch (e: unknown) {
    const detail = getErrorMessage(e);
    console.error('Error:', detail);
    set({ error: `Erro ao adicionar √† base: ${detail}` });
  }
},
```

### 3. Fun√ß√£o Helper para Chunking

```typescript
// utils/textChunking.ts
export function chunkText(
  text: string,
  maxChunkSize: number = 500,
  overlap: number = 50
): { text: string; index: number }[] {
  const chunks: { text: string; index: number }[] = [];
  let index = 0;
  let position = 0;

  while (position < text.length) {
    const end = Math.min(position + maxChunkSize, text.length);
    const chunk = text.slice(position, end);
    
    chunks.push({ text: chunk.trim(), index });
    
    position = end - overlap;
    index++;
  }

  return chunks;
}
```

---

## üîÑ Fluxo Completo

### Fluxo 1: Upload e Processamento

```
1. Usu√°rio faz upload de PDF
   ‚Üì
2. Frontend extrai texto
   ‚Üì
3. Salva em `documents` table
   ‚Üì
4. Usu√°rio clica "Adicionar √† Base"
   ‚Üì
5. Backend divide texto em chunks
   ‚Üì
6. Para cada chunk:
   - Gera embedding via OpenAI
   - Salva em `document_embeddings`
   ‚Üì
7. Documento marcado como `is_deletable = false`
   ‚Üì
8. Toast de sucesso exibido
```

### Fluxo 2: Chat com RAG

```
1. Usu√°rio envia mensagem
   ‚Üì
2. Gera embedding da mensagem
   ‚Üì
3. Busca chunks similares (match_document_chunks)
   ‚Üì
4. Ordena por similaridade (cosine)
   ‚Üì
5. Pega top 5 chunks mais relevantes
   ‚Üì
6. Injeta chunks como contexto no prompt
   ‚Üì
7. Envia para LLM (OpenRouter/Gemini)
   ‚Üì
8. LLM responde baseado no contexto
   ‚Üì
9. Resposta exibida ao usu√°rio
```

---

## üß™ Testes

### 1. Testar Embedding

```typescript
// test/embed.test.ts
const response = await fetch('/api/embed', {
  method: 'POST',
  body: JSON.stringify({ text: 'Como calcular ICMS?' }),
});
const { embedding } = await response.json();
console.log('Embedding dimensions:', embedding.length); // Should be 1536
```

### 2. Testar Busca Sem√¢ntica

```sql
-- No Supabase SQL Editor
SELECT * FROM match_document_chunks(
  '[0.1, 0.2, ...]'::vector(1536), -- embedding de teste
  0.7, -- threshold
  5    -- limit
);
```

### 3. Testar RAG Completo

```typescript
// Enviar mensagem e verificar se contexto foi usado
const message = "O que √© SPED Fiscal?";
// Verificar se a resposta menciona informa√ß√µes dos documentos
```

---

## üìä Monitoramento

### Queries √öteis

```sql
-- Ver documentos com embeddings
SELECT * FROM documents_with_chunk_count;

-- Ver PDFs processados
SELECT * FROM pdf_documents_with_stats;

-- Contar embeddings por documento
SELECT 
  d.name,
  COUNT(de.id) as total_chunks,
  COUNT(de.embedding) as embedded_chunks
FROM documents d
LEFT JOIN document_embeddings de ON d.id = de.document_id
GROUP BY d.id, d.name;

-- Ver chunks mais similares a um texto
SELECT 
  chunk_text,
  similarity
FROM match_document_chunks(
  (SELECT embedding FROM document_embeddings LIMIT 1),
  0.5,
  10
);
```

---

## üöÄ Pr√≥ximos Passos

1. ‚úÖ **Implementar chunking inteligente** (por par√°grafo, senten√ßa)
2. ‚úÖ **Cache de embeddings** (evitar reprocessamento)
3. ‚úÖ **Hybrid search** (keyword + semantic)
4. ‚úÖ **Re-ranking** dos resultados
5. ‚úÖ **Metadata filtering** (data, tipo, etc)
6. ‚úÖ **Analytics** de uso do RAG

---

## üìö Recursos

- [Supabase Vector Docs](https://supabase.com/docs/guides/ai/vector-columns)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)
- [pgvector GitHub](https://github.com/pgvector/pgvector)
- [HNSW Algorithm](https://arxiv.org/abs/1603.09320)

---

**Criado por:** Genesis AI Team  
**Data:** 2025-11-23  
**Vers√£o:** 1.0.0
